{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Code and documentation written by group members***\n",
    "\n",
    "Model name : RNN1\n",
    "\n",
    "Type of Classification: Binary classification - Stress and Non Stress\n",
    "\n",
    "**1. Loading the EEG data samples and its labels**\n",
    "\n",
    "-   The dataset folder consists of an Excel file called \"scales.xls\"\n",
    "    which consists of the subjects' stress level rating from 1-10. We\n",
    "    consider this as the labels for the corresponding EEG samples from\n",
    "    the dataset.\n",
    "\n",
    "-   For example, if we load an EEG sample of subject 10 from the\n",
    "    dataset, his/her stress rating for that mental task is used as the\n",
    "    EEG sample's label.\n",
    "\n",
    "-   The EEG samples file names are formatted in the following manner:\n",
    "    MentalTaskType_sub_no_trial_no.mat - for example:\n",
    "    Relax_sub_2\\_trial2.mat\n",
    "\n",
    "    -   Mental Task Type : either Arithmetic, Stroop, Mirror_Image or\n",
    "        Relax\n",
    "    -   sub_no : Subject Number\n",
    "    -   trial_no : Trial Number\n",
    "\n",
    "    The functions in the following code block and their functionalities:\n",
    "\n",
    "    -   extract_number(file_name_snippet) : extracts the trial number\n",
    "        from the given filename snippet\n",
    "    -   extract_label_address(file_name) : divides the file name into 4\n",
    "        sections and extracts the task type, subject number and trial\n",
    "        number. The function uses this information to calculate and\n",
    "        return the label address on the Excel sheet. The Relax EEG\n",
    "        samples do not have ratings on the Excel sheet, they\n",
    "        automatically return stress rating 0.\n",
    "    -   diff_label(original_stress_label) : the label from the Excel\n",
    "        sheet ranges from 1 to 10. However, we want to group the labels\n",
    "        to either 0 or 1, as we are performing binary classification. So\n",
    "        the EEG samples with label from 1 to 10 were mapped to the\n",
    "        stress label 1 (to signify stress) and the EEG samples which\n",
    "        were from files labelled as \"Relax\" with stress rating 0 were\n",
    "        mapped to stress label 0 (to signify non-stress)\n",
    "    -   read_data(file_array) : takes in a string array of filepaths of\n",
    "        all the EEG sample files and extracts the EEG coordinates from\n",
    "        each filepath. Stores each file's EEG coordinates in an array\n",
    "        and gets its corresponding label using extract_label_address().\n",
    "        This function returns two arrays: an array storing arrays of\n",
    "        each files' EEG coordinates called all_samples and an array\n",
    "        called all_labels, which stores each file's corresponding label\n",
    "        to feed into the model.\n",
    "\n",
    "In \\[1\\]:\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from glob import glob\n",
    "    import matplotlib.pyplot as plt\n",
    "    import scipy.io\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    # extract numbers for labels in later stages\n",
    "    def extract_number(string):\n",
    "        for char in string:\n",
    "            if char.isdigit():\n",
    "                return int(char)\n",
    "        return None\n",
    "\n",
    "    # takes filepath as parameter, returns indices for address of stress level label in xls file\n",
    "    def extract_label_address(string):\n",
    "        arr=string.split(os.path.sep)[-1].split(\"_\") # ['Arithmetic', 'sub', '10', 'trial1.mat']\n",
    "        base_num=0\n",
    "        if (\"Arithmetic\" in arr[0]):\n",
    "            base_num=1\n",
    "        elif (\"Mirror\" in arr[0]):\n",
    "            base_num=2\n",
    "        elif (\"Stroop\" in arr[0]):\n",
    "            base_num=3\n",
    "        else:\n",
    "            return 0,0\n",
    "        trial_no=extract_number(arr[-1])\n",
    "        if trial_no==2:\n",
    "            base_num+=3\n",
    "        elif trial_no==3:\n",
    "            base_num+=6\n",
    "        return int(arr[-2]),base_num # returns address of cell in excel file\n",
    "\n",
    "    # all samples other than Relax are mapped to label 1 (Stress)\n",
    "    def diff_label(stress_label):\n",
    "        if stress_label==0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        return stress_label\n",
    "\n",
    "    # total 480 files, 3 trials for each of the 40 participants in each of the 4 experiments\n",
    "    all_file_paths=sorted(glob(\"filtered_data/*.mat\"))\n",
    "\n",
    "    # reading excel file's stress level labels - arr stores the full excel file's contents\n",
    "    stress_levels_arr = pd.read_excel('Dataset_Information/scales.xls').to_numpy()\n",
    "\n",
    "    # reads the data and returns the samples array and its label array\n",
    "    def read_data(file_paths):\n",
    "        data = scipy.io.loadmat(file_paths[0])\n",
    "        eeg_data = data[\"Clean_data\"]\n",
    "        all_samples=[eeg_data]\n",
    "        addr1,addr2=extract_label_address(file_paths[0])\n",
    "        all_labels=[diff_label(stress_levels_arr[addr1][addr2])]\n",
    "        for i in range(1,len(file_paths)):\n",
    "            data = scipy.io.loadmat(file_paths[i])\n",
    "            eeg_data = data[\"Clean_data\"]\n",
    "            all_samples=np.append(all_samples,[eeg_data],axis=0)\n",
    "            addr1,addr2=extract_label_address(file_paths[i])\n",
    "            if (addr1!=0 and addr2!=0): # if addr1, addr2 == 0,0: we are labelling a Relaxation file\n",
    "                all_labels=np.append(all_labels,[diff_label(stress_levels_arr[addr1][addr2])],axis=0)\n",
    "            else:\n",
    "                all_labels=np.append(all_labels,[diff_label(0)],axis=0)\n",
    "        return all_samples,all_labels\n",
    "\n",
    "Structure of all_samples array: Its shape is (480,32,3200) signifying\n",
    "480 files, each containing 32 channels worth of 3200 time samples\n",
    "\n",
    "In \\[3\\]:\n",
    "\n",
    "    # total 480 files\n",
    "    # each file is of shape (32,3200), total 120 files per experiment\n",
    "    # test if labels correct - should be able to access both the EEG sample and its label from same index in both arrays\n",
    "    addr1,addr2=extract_label_address(all_file_paths[304])\n",
    "    print(\"file_name: \",all_file_paths[304])\n",
    "    print(\"label: \", stress_levels_arr[addr1][addr2])\n",
    "\n",
    "    file_name:  filtered_data\\Relax_sub_2_trial2.mat\n",
    "    label:  0\n",
    "\n",
    "In \\[4\\]:\n",
    "\n",
    "    all_samples,all_labels=read_data(all_file_paths) # shape should be (480,32,3200) and (480,) respectively\n",
    "\n",
    "**2. Preprocessing Data for RNN Model**\n",
    "\n",
    "-   Although the EEG data samples are already preprocessed in the sense\n",
    "    of artifact and noise removal, there are additional preprocessing\n",
    "    steps which include:\n",
    "    -   oversampling to balance the classes (1/5th of the dataset was\n",
    "        non-stressed EEG samples, so oversampling was done to compensate\n",
    "        for the class imbalance)\n",
    "    -   splitting the data into training and testing sets (80% of the\n",
    "        dataset was used for training, 20% was used for testing)\n",
    "\n",
    "**3. Architecture of the RNN Model**\n",
    "\n",
    "-   Input Layer: The input shape for the RNN model is specified as (32,\n",
    "    3200), indicating that each EEG sample from the training set\n",
    "    consists of 32 channels with 3200 time samples each.\n",
    "    -   Recurrent Layers:\n",
    "        -   The model consists of two SimpleRNN layers:\n",
    "        -   The first SimpleRNN layer has 128 units and uses the ReLU\n",
    "            activation function.\n",
    "        -   The second SimpleRNN layer has 64 units and also uses the\n",
    "            ReLU activation function.\n",
    "        -   Both layers have a kernel regularization term with L2\n",
    "            regularization strength of 0.01.\n",
    "        -   The first layer is set to return sequences, indicating that\n",
    "            it returns the output for each time step.\n",
    "    -   Flattening Layer: After the recurrent layers, the output is\n",
    "        flattened using a Flatten layer. This converts the 2D output\n",
    "        from the recurrent layers into a 1D array.\n",
    "    -   Dense Layers: After the Flatten layer, there is a Dense layer\n",
    "        with 64 units and ReLU activation function.\n",
    "    -   Output Layer: The final output layer is a Dense layer with 1\n",
    "        unit and sigmoid activation function. The output is a\n",
    "        probability score indicating the likelihood of the sample\n",
    "        belonging to the positive class (stressed).\n",
    "\n",
    "**4. Compilation**\n",
    "\n",
    "The model is compiled using the Adam optimizer and binary crossentropy\n",
    "loss function. Accuracy is used as the evaluation metric.\n",
    "\n",
    "**5. Training the Model**\n",
    "\n",
    "The model is trained for 20 epochs with a batch size of 32. The training\n",
    "progress is monitored, and validation data is used to evaluate the\n",
    "model's performance during training.\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import h5py\n",
    "\n",
    "    # Assuming all_samples shape is (480, 32, 3200)\n",
    "    # Assuming all_labels shape is (480,)\n",
    "\n",
    "    # identify indices of non-stress class\n",
    "    non_stress_indices = np.where(all_labels == 0)[0]\n",
    "\n",
    "    # oversampling factor\n",
    "    oversampling_factor = 4\n",
    "\n",
    "    # oversampling the non-stress samples\n",
    "    replicated_samples = np.repeat(all_samples[non_stress_indices], oversampling_factor, axis=0)\n",
    "    replicated_labels = np.repeat(all_labels[non_stress_indices], oversampling_factor)\n",
    "\n",
    "    # combine with original data\n",
    "    oversampled_samples = np.vstack([all_samples, replicated_samples]) # 960,2,3200\n",
    "    oversampled_labels = np.concatenate([all_labels, replicated_labels])\n",
    "\n",
    "    # split oversampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(oversampled_samples, oversampled_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # define RNN model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # add a SimpleRNN layer with return_sequences=True for multiple time steps\n",
    "    model.add(layers.SimpleRNN(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=(32, 3200), return_sequences=True))\n",
    "\n",
    "    # additional SimpleRNN layer\n",
    "    model.add(layers.SimpleRNN(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "\n",
    "    # flatten output before passing it to next dense layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # extra dense layer\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "    # final output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # train model\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "    # evaluate model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = np.round(y_pred)\n",
    "\n",
    "    Epoch 1/20\n",
    "    24/24 [==============================] - 19s 401ms/step - loss: 5.0068 - accuracy: 0.7096 - val_loss: 3.6336 - val_accuracy: 0.8958\n",
    "    Epoch 2/20\n",
    "    24/24 [==============================] - 5s 222ms/step - loss: 3.4602 - accuracy: 0.9805 - val_loss: 3.4930 - val_accuracy: 0.9375\n",
    "    Epoch 3/20\n",
    "    24/24 [==============================] - 3s 148ms/step - loss: 3.3492 - accuracy: 1.0000 - val_loss: 3.4088 - val_accuracy: 0.9375\n",
    "    Epoch 4/20\n",
    "    24/24 [==============================] - 3s 146ms/step - loss: 3.2681 - accuracy: 1.0000 - val_loss: 3.3251 - val_accuracy: 0.9427\n",
    "    Epoch 5/20\n",
    "    24/24 [==============================] - 4s 150ms/step - loss: 3.1758 - accuracy: 1.0000 - val_loss: 3.2270 - val_accuracy: 0.9427\n",
    "    Epoch 6/20\n",
    "    24/24 [==============================] - 4s 149ms/step - loss: 3.0759 - accuracy: 1.0000 - val_loss: 3.1219 - val_accuracy: 0.9427\n",
    "    Epoch 7/20\n",
    "    24/24 [==============================] - 3s 145ms/step - loss: 2.9704 - accuracy: 1.0000 - val_loss: 3.0109 - val_accuracy: 0.9479\n",
    "    Epoch 8/20\n",
    "    24/24 [==============================] - 3s 142ms/step - loss: 2.8610 - accuracy: 1.0000 - val_loss: 2.8977 - val_accuracy: 0.9531\n",
    "    Epoch 9/20\n",
    "    24/24 [==============================] - 3s 139ms/step - loss: 2.7490 - accuracy: 1.0000 - val_loss: 2.7825 - val_accuracy: 0.9531\n",
    "    Epoch 10/20\n",
    "    24/24 [==============================] - 3s 141ms/step - loss: 2.6354 - accuracy: 1.0000 - val_loss: 2.6662 - val_accuracy: 0.9531\n",
    "    Epoch 11/20\n",
    "    24/24 [==============================] - 3s 140ms/step - loss: 2.5213 - accuracy: 1.0000 - val_loss: 2.5504 - val_accuracy: 0.9479\n",
    "    Epoch 12/20\n",
    "    24/24 [==============================] - 3s 138ms/step - loss: 2.4075 - accuracy: 1.0000 - val_loss: 2.4354 - val_accuracy: 0.9479\n",
    "    Epoch 13/20\n",
    "    24/24 [==============================] - 3s 144ms/step - loss: 2.2946 - accuracy: 1.0000 - val_loss: 2.3216 - val_accuracy: 0.9479\n",
    "    Epoch 14/20\n",
    "    24/24 [==============================] - 3s 138ms/step - loss: 2.1833 - accuracy: 1.0000 - val_loss: 2.2097 - val_accuracy: 0.9479\n",
    "    Epoch 15/20\n",
    "    24/24 [==============================] - 3s 146ms/step - loss: 2.0740 - accuracy: 1.0000 - val_loss: 2.0998 - val_accuracy: 0.9531\n",
    "    Epoch 16/20\n",
    "    24/24 [==============================] - 3s 133ms/step - loss: 1.9672 - accuracy: 1.0000 - val_loss: 1.9929 - val_accuracy: 0.9479\n",
    "    Epoch 17/20\n",
    "    24/24 [==============================] - 4s 168ms/step - loss: 1.8632 - accuracy: 1.0000 - val_loss: 1.8889 - val_accuracy: 0.9479\n",
    "    Epoch 18/20\n",
    "    24/24 [==============================] - 4s 179ms/step - loss: 1.7623 - accuracy: 1.0000 - val_loss: 1.7903 - val_accuracy: 0.9427\n",
    "    Epoch 19/20\n",
    "    24/24 [==============================] - 4s 156ms/step - loss: 1.6647 - accuracy: 1.0000 - val_loss: 1.6945 - val_accuracy: 0.9427\n",
    "    Epoch 20/20\n",
    "    24/24 [==============================] - 3s 137ms/step - loss: 1.5706 - accuracy: 1.0000 - val_loss: 1.6016 - val_accuracy: 0.9427\n",
    "    6/6 [==============================] - 1s 41ms/step - loss: 1.6016 - accuracy: 0.9427\n",
    "    Test Accuracy: 94.27%\n",
    "    6/6 [==============================] - 5s 38ms/step\n",
    "\n",
    "**6. Results of Testing the Model - confusion matrix and classification\n",
    "report**\n",
    "\n",
    "Report generated below provides a summary of various classification\n",
    "metrics such as precision, recall, F1-score, and support for each class.\n",
    "This reports the evaluation of testing using 20% of the data from the\n",
    "dataset.\n",
    "\n",
    "In \\[15\\]:\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # output confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # generate and print the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "    Confusion Matrix:\n",
    "    [[133   0]\n",
    "     [ 11  48]]\n",
    "\n",
    "    Classification Report:\n",
    "                  precision    recall  f1-score   support\n",
    "\n",
    "               0       0.92      1.00      0.96       133\n",
    "               1       1.00      0.81      0.90        59\n",
    "\n",
    "        accuracy                           0.94       192\n",
    "       macro avg       0.96      0.91      0.93       192\n",
    "    weighted avg       0.95      0.94      0.94       192\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "    # simple testing if the trained model predicts correctly\n",
    "    import numpy as np\n",
    "    from scipy.io import loadmat\n",
    "    test_index=203 # try different file path indexes\n",
    "    # Load EEG data from the .mat file - example\n",
    "    new_user_input = loadmat(all_file_paths[test_index])\n",
    "    eeg_data = new_user_input[\"Clean_data\"]\n",
    "    input_for_model = eeg_data\n",
    "    # add a batch dimension before feeding into model\n",
    "    input_for_model = np.expand_dims(input_for_model, axis=0)\n",
    "    prediction = model.predict(input_for_model)\n",
    "\n",
    "    # Round prediction to get binary output\n",
    "    binary_prediction = np.round(prediction)\n",
    "    print(\"Actual Label:\", all_labels[test_index])\n",
    "    print(\"Model's prediction:\", binary_prediction[0][0])\n",
    "\n",
    "    1/1 [==============================] - 1s 822ms/step\n",
    "    Actual Label: 1\n",
    "    Model's prediction: 1.0\n",
    "\n",
    "**7. Saving the Trained Model**\n",
    "\n",
    "This action ensures that the trained model can be easily retrieved and\n",
    "reused for future predictions or analysis without the need to retrain\n",
    "it. The .h5 version will be used when the model is required to classify\n",
    "a user's EEG sample for the website.\n",
    "\n",
    "In \\[16\\]:\n",
    "\n",
    "    import h5py\n",
    "    # Save the trained model to an HDF5 file\n",
    "    model.save(\"C:/Users/Manoharan/Desktop/CorrectedRNN1.h5\")"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
