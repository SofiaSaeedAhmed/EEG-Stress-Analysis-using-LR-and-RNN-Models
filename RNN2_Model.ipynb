{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Code and documentation written by group members***\n",
    "\n",
    "Model name : RNN2\n",
    "\n",
    "Type of Classification: Binary classification - Low Stress and High\n",
    "Stress\n",
    "\n",
    "-   In our application, the user's EEG sample is first fed through RNN1\n",
    "    to classify the sample as either \"stress\" or \"non-stress\"\n",
    "-   If the sample is classified as \"stress\" then the EEG sample is fed\n",
    "    into RNN2 to classify the EEG sample as either \"low stress\" or \"high\n",
    "    stress\".\n",
    "\n",
    "**1. Loading the EEG data samples and its labels**\n",
    "\n",
    "-   The loading of the EEG sample files and labels is the same as for\n",
    "    model RNN1. Please refer to RNN1_Model_Documentation.html for\n",
    "    detailed explanation about each function in the following code block\n",
    "    which performs the loading of EEG sample files and labels.\n",
    "\n",
    "In \\[1\\]:\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from glob import glob\n",
    "    import matplotlib.pyplot as plt\n",
    "    import scipy.io\n",
    "    import mne\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    # for labels in later stages\n",
    "    def extract_number(string):\n",
    "        for char in string:\n",
    "            if char.isdigit():\n",
    "                return int(char)\n",
    "        return None\n",
    "\n",
    "    # takes filepath as parameter, returns indices for address of stress level label in xls file\n",
    "    def extract_label_address(string):\n",
    "        arr=string.split(os.path.sep)[-1].split(\"_\") # ['Arithmetic', 'sub', '10', 'trial1.mat']\n",
    "        # ^ elem 0: which type of test\n",
    "        # elem 2: subject number\n",
    "        # extract_number(elem 3): trial number\n",
    "        base_num=0\n",
    "        if (\"Arithmetic\" in arr[0]):\n",
    "            base_num=1\n",
    "        elif (\"Mirror\" in arr[0]):\n",
    "            base_num=2\n",
    "        elif (\"Stroop\" in arr[0]):\n",
    "            base_num=3\n",
    "        else:\n",
    "            return 0,0\n",
    "        # else remains 0, for Relaxation\n",
    "        trial_no=extract_number(arr[-1])\n",
    "        if trial_no==2:\n",
    "            base_num+=3\n",
    "        elif trial_no==3:\n",
    "            base_num+=6\n",
    "        return int(arr[-2]),base_num # returns address of cell in excel file\n",
    "\n",
    "    # mapping the stress labels to either 0 or 1\n",
    "    def diff_label(stress_label):\n",
    "        if stress_label>=1 and stress_label<=5: # if subject rating from 0 to 5, label as low stress (0)\n",
    "            stress_label=0\n",
    "        else:\n",
    "            stress_label=1 # if subject rating is from 6 to 10, label as high stress (1)\n",
    "        return stress_label\n",
    "\n",
    "    # reads the data and returns the samples array and its label array\n",
    "    def read_data(file_paths):\n",
    "        # load first element\n",
    "        data = scipy.io.loadmat(file_paths[0])\n",
    "        eeg_data = data[\"Clean_data\"]\n",
    "        all_samples=[eeg_data]\n",
    "        addr1,addr2=extract_label_address(file_paths[0])\n",
    "        all_labels=[diff_label(stress_levels_arr[addr1][addr2])]\n",
    "        for i in range(1,len(file_paths)):\n",
    "            data = scipy.io.loadmat(file_paths[i])# loads EEG coordinates from .mat file\n",
    "            eeg_data = data[\"Clean_data\"] # get EEG data in numpy array, has 32 channels, and 3200 time samples\n",
    "            all_samples=np.append(all_samples,[eeg_data],axis=0)\n",
    "            # extract the stress level for that one file of EEG data\n",
    "            addr1,addr2=extract_label_address(file_paths[i])\n",
    "            # append the corresponding label of that EEG sample file to all_labels\n",
    "            all_labels=np.append(all_labels,[diff_label(stress_levels_arr[addr1][addr2])],axis=0)\n",
    "        return all_samples,all_labels\n",
    "\n",
    "The dataset contains some EEG samples called \"Relax\", which represents\n",
    "the non-stress EEG samples. RNN2 is only required to differentiate\n",
    "between low stress and high stress EEG samples, therefore the filepaths\n",
    "to the Relax EEG samples are removed.\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    # total 480 files, 3 trials for each of the 40 participants in each of the 4 experiments\n",
    "    all_file_paths=sorted(glob(\"filtered_data/*.mat\"))\n",
    "    # Filter out file names containing the word \"Relax\" - we are not using non-stress EEG samples\n",
    "    filtered_file_paths = [file_path for file_path in all_file_paths if \"Relax\" not in file_path]\n",
    "    all_file_paths=filtered_file_paths # now total 360 file paths of EEG samples, excluding Relax files \n",
    "\n",
    "In \\[3\\]:\n",
    "\n",
    "    # reading excel file's stress level labels\n",
    "    stress_levels_arr = pd.read_excel('Dataset_Information/scales.xls').to_numpy()\n",
    "\n",
    "    # testing if labels are correct\n",
    "    addr1,addr2=extract_label_address(all_file_paths[349])\n",
    "\n",
    "    # each file's content is of shape (32,3200), total 120 EEG samples per experiment\n",
    "    # we do not use the relaxation EEG samples to train the model, so 360 files will be used, not 480\n",
    "    # load the EEG data and its labels\n",
    "    all_samples,all_labels=read_data(all_file_paths) # should be (360,32,3200) and (360,)\n",
    "\n",
    "After loading the EEG samples and their corresponding labels, we can\n",
    "check if the EEG sample and its corresponding label can be accessed with\n",
    "the same index. This ensures that the EEG samples and their respective\n",
    "labels are loaded accurately.\n",
    "\n",
    "In \\[4\\]:\n",
    "\n",
    "    # test if labels match the EEG sample\n",
    "    print(all_samples.shape,all_labels.shape) # should be (360,32,3200) and (360,)\n",
    "    print(\"File name: \",all_file_paths[3])\n",
    "    print(\"Label: \",all_labels[3])\n",
    "\n",
    "    (360, 32, 3200) (360,)\n",
    "    File name:  filtered_data\\Arithmetic_sub_11_trial1.mat\n",
    "    Label:  1\n",
    "\n",
    "**2. Preprocessing Data for RNN2 Model**\n",
    "\n",
    "-   Although the EEG data samples are already preprocessed in the sense\n",
    "    of artifact and noise removal, there are additional preprocessing\n",
    "    steps which include:\n",
    "    -   oversampling to balance the classes ( There are 233 low-stress\n",
    "        EEG samples and 127 high-stress EEG samples. Therefore random\n",
    "        oversampling had to be done for high-stress EEG samples to\n",
    "        compensate for the class imbalance.)\n",
    "    -   splitting the data into training and testing sets (80% of the\n",
    "        dataset was used for training, 20% was used for testing)\n",
    "\n",
    "**3. Architecture of the RNN2 Model**\n",
    "\n",
    "-   RNN2 uses the same architecture as RNN1 - refer to\n",
    "    RNN1_Model_Documentation.html\n",
    "\n",
    "**4. Compilation**\n",
    "\n",
    "-   RNN2 has the same compilation process as RNN1 - refer to\n",
    "    RNN1_Model_Documentation.html\n",
    "\n",
    "**5. Training the Model**\n",
    "\n",
    "-   RNN2 shares the same parameter settings and training methodologies\n",
    "    as RNN1 - refer to RNN1_Model_Documentation.html\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import h5py\n",
    "\n",
    "    # Assuming all_samples shape is (360, 32, 3200)\n",
    "    # Assuming all_labels shape is (360,)\n",
    "\n",
    "    # Identify indices of high-stress class\n",
    "    high_stress_indices = np.where(all_labels == 1)[0]\n",
    "\n",
    "    # oversampling factor - controls quantity of oversampling\n",
    "    oversampling_factor = 4\n",
    "\n",
    "    # Replicate high-stress samples\n",
    "    replicated_samples = np.repeat(all_samples[high_stress_indices], oversampling_factor, axis=0)\n",
    "    replicated_labels = np.repeat(all_labels[high_stress_indices], oversampling_factor)\n",
    "\n",
    "    # combine with original data\n",
    "    oversampled_samples = np.vstack([all_samples, replicated_samples])\n",
    "    oversampled_labels = np.concatenate([all_labels, replicated_labels])\n",
    "\n",
    "    # split oversampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(oversampled_samples, oversampled_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # define RNN model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # add a SimpleRNN layer with return_sequences=True for multiple time steps\n",
    "    model.add(layers.SimpleRNN(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=(32, 3200), return_sequences=True))\n",
    "\n",
    "    # additional SimpleRNN layer\n",
    "    model.add(layers.SimpleRNN(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "\n",
    "    # flatten output before passing it to dense layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # add Dense layer\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "    # add final output layer - gives probability of which class it belongs to\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # train model\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = np.round(y_pred)\n",
    "\n",
    "    Epoch 1/20\n",
    "    22/22 [==============================] - 17s 382ms/step - loss: 4.8092 - accuracy: 0.7277 - val_loss: 3.6081 - val_accuracy: 0.9195\n",
    "    Epoch 2/20\n",
    "    22/22 [==============================] - 5s 230ms/step - loss: 3.4331 - accuracy: 0.9798 - val_loss: 3.4741 - val_accuracy: 0.9483\n",
    "    Epoch 3/20\n",
    "    22/22 [==============================] - 4s 208ms/step - loss: 3.2994 - accuracy: 1.0000 - val_loss: 3.3737 - val_accuracy: 0.9483\n",
    "    Epoch 4/20\n",
    "    22/22 [==============================] - 4s 197ms/step - loss: 3.1997 - accuracy: 1.0000 - val_loss: 3.2760 - val_accuracy: 0.9425\n",
    "    Epoch 5/20\n",
    "    22/22 [==============================] - 3s 156ms/step - loss: 3.0868 - accuracy: 1.0000 - val_loss: 3.1609 - val_accuracy: 0.9368\n",
    "    Epoch 6/20\n",
    "    22/22 [==============================] - 3s 146ms/step - loss: 2.9653 - accuracy: 1.0000 - val_loss: 3.0357 - val_accuracy: 0.9368\n",
    "    Epoch 7/20\n",
    "    22/22 [==============================] - 3s 156ms/step - loss: 2.8382 - accuracy: 1.0000 - val_loss: 2.9055 - val_accuracy: 0.9368\n",
    "    Epoch 8/20\n",
    "    22/22 [==============================] - 3s 157ms/step - loss: 2.7075 - accuracy: 1.0000 - val_loss: 2.7708 - val_accuracy: 0.9368\n",
    "    Epoch 9/20\n",
    "    22/22 [==============================] - 3s 148ms/step - loss: 2.5750 - accuracy: 1.0000 - val_loss: 2.6374 - val_accuracy: 0.9425\n",
    "    Epoch 10/20\n",
    "    22/22 [==============================] - 3s 146ms/step - loss: 2.4423 - accuracy: 1.0000 - val_loss: 2.5031 - val_accuracy: 0.9425\n",
    "    Epoch 11/20\n",
    "    22/22 [==============================] - 3s 150ms/step - loss: 2.3105 - accuracy: 1.0000 - val_loss: 2.3710 - val_accuracy: 0.9425\n",
    "    Epoch 12/20\n",
    "    22/22 [==============================] - 3s 154ms/step - loss: 2.1806 - accuracy: 1.0000 - val_loss: 2.2407 - val_accuracy: 0.9483\n",
    "    Epoch 13/20\n",
    "    22/22 [==============================] - 3s 153ms/step - loss: 2.0535 - accuracy: 1.0000 - val_loss: 2.1129 - val_accuracy: 0.9483\n",
    "    Epoch 14/20\n",
    "    22/22 [==============================] - 3s 158ms/step - loss: 1.9299 - accuracy: 1.0000 - val_loss: 1.9933 - val_accuracy: 0.9483\n",
    "    Epoch 15/20\n",
    "    22/22 [==============================] - 3s 149ms/step - loss: 1.8103 - accuracy: 1.0000 - val_loss: 1.8732 - val_accuracy: 0.9483\n",
    "    Epoch 16/20\n",
    "    22/22 [==============================] - 3s 152ms/step - loss: 1.6951 - accuracy: 1.0000 - val_loss: 1.7601 - val_accuracy: 0.9483\n",
    "    Epoch 17/20\n",
    "    22/22 [==============================] - 3s 149ms/step - loss: 1.5846 - accuracy: 1.0000 - val_loss: 1.6521 - val_accuracy: 0.9483\n",
    "    Epoch 18/20\n",
    "    22/22 [==============================] - 3s 161ms/step - loss: 1.4790 - accuracy: 1.0000 - val_loss: 1.5498 - val_accuracy: 0.9483\n",
    "    Epoch 19/20\n",
    "    22/22 [==============================] - 4s 196ms/step - loss: 1.3785 - accuracy: 1.0000 - val_loss: 1.4536 - val_accuracy: 0.9368\n",
    "    Epoch 20/20\n",
    "    22/22 [==============================] - 5s 216ms/step - loss: 1.2831 - accuracy: 1.0000 - val_loss: 1.3618 - val_accuracy: 0.9310\n",
    "    6/6 [==============================] - 1s 42ms/step - loss: 1.3618 - accuracy: 0.9310\n",
    "    Test Accuracy: 93.10%\n",
    "    6/6 [==============================] - 5s 33ms/step\n",
    "\n",
    "**6. Results of Testing the Model - confusion matrix and classification\n",
    "report**\n",
    "\n",
    "Report generated below provides a summary of various classification\n",
    "metrics such as precision, recall, F1-score, and support for each class.\n",
    "This reports the evaluation of testing using 20% of the data from the\n",
    "dataset.\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # output confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "    print(conf_matrix)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "    Confusion Matrix:\n",
    "    [[ 37  12]\n",
    "     [  0 125]]\n",
    "\n",
    "    Classification Report:\n",
    "                  precision    recall  f1-score   support\n",
    "\n",
    "               0       1.00      0.76      0.86        49\n",
    "               1       0.91      1.00      0.95       125\n",
    "\n",
    "        accuracy                           0.93       174\n",
    "       macro avg       0.96      0.88      0.91       174\n",
    "    weighted avg       0.94      0.93      0.93       174\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    # simple testing if the trained model predicts correctly\n",
    "    import numpy as np\n",
    "    from scipy.io import loadmat\n",
    "\n",
    "    test_index=3\n",
    "    # load EEG data from .mat file - example:\n",
    "    new_user_input = loadmat(all_file_paths[test_index])\n",
    "    eeg_data = new_user_input[\"Clean_data\"]\n",
    "    input_for_model = eeg_data\n",
    "\n",
    "    # Add a batch dimension\n",
    "    input_for_model = np.expand_dims(input_for_model, axis=0)\n",
    "\n",
    "    # perform prediction\n",
    "    prediction = model.predict(input_for_model)\n",
    "\n",
    "    # round prediction to get binary output\n",
    "    binary_prediction = np.round(prediction)\n",
    "\n",
    "    print(\"Actual Label:\", all_labels[test_index])\n",
    "    print(\"Binary prediction:\", binary_prediction[0][0])\n",
    "\n",
    "    1/1 [==============================] - 2s 2s/step\n",
    "    Actual Label: 1\n",
    "    Binary prediction: 1.0\n",
    "\n",
    "**7. Saving the Trained Model**\n",
    "\n",
    "This action ensures that the trained model can be easily retrieved and\n",
    "reused for future predictions or analysis without the need to retrain\n",
    "it. The .h5 version will be used when the model is required to classify\n",
    "a user's EEG sample for the website.\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    import h5py\n",
    "    # Save the trained model to an HDF5 file\n",
    "    model.save(\"C:/Users/Manoharan/Desktop/CorrectedRNN2.h5\")"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
