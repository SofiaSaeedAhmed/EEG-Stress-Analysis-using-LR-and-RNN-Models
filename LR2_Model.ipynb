{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Code and documentation written by group members***\n",
    "\n",
    "Model name : LR2\n",
    "\n",
    "Type of Classification: Binary classification - Low Stress and High\n",
    "Stress\n",
    "\n",
    "-   In our application, the user's EEG sample is first fed through LR1\n",
    "    to classify the sample as either \"stress\" or \"non-stress\"\n",
    "-   If the sample is classified as \"stress\" then the EEG sample is fed\n",
    "    into LR2 to classify the EEG sample as either \"low stress\" or \"high\n",
    "    stress\".\n",
    "\n",
    "**1. Loading the EEG data samples and its labels**\n",
    "\n",
    "-   The loading of the EEG sample files and labels is the same as for\n",
    "    model LR1. Please refer to LR1_Model_Documentation.html for detailed\n",
    "    explanation about each function in the following code block which\n",
    "    performs the loading of EEG sample files and labels.\n",
    "\n",
    "In \\[4\\]:\n",
    "\n",
    "    import numpy as np  # for numerical operations\n",
    "    from sklearn.model_selection import train_test_split # split data in training and testing\n",
    "    from sklearn.preprocessing import StandardScaler  # standardizing feature values\n",
    "    from glob import glob  # file path expansion\n",
    "    import matplotlib.pyplot as plt   # plotting\n",
    "    import scipy.io    # reading MATLAB files\n",
    "    import os     # OS related functionalities\n",
    "    import pandas as pd   # handle excel data in tabular form\n",
    "    from sklearn.linear_model import LogisticRegression #Import the Logistic Regression model from scikit-learn\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    import seaborn as sns\n",
    "\n",
    "\n",
    "    # for labels in later stages\n",
    "    def extract_number(string):\n",
    "        for char in string:\n",
    "            if char.isdigit():\n",
    "                return int(char)\n",
    "        return None\n",
    "\n",
    "    # takes filepath as parameter, returns indices for address of stress level label in xls file\n",
    "    def extract_label_address(string):\n",
    "        arr=string.split(os.path.sep)[-1].split(\"_\") # ['Arithmetic', 'sub', '10', 'trial1.mat']\n",
    "        # ^ elem 0: which type of test\n",
    "        # elem 2: subject number\n",
    "        # extract_number(elem 3): trial number\n",
    "        base_num=0\n",
    "        if (\"Arithmetic\" in arr[0]):\n",
    "            base_num=1\n",
    "        elif (\"Mirror\" in arr[0]):\n",
    "            base_num=2\n",
    "        elif (\"Stroop\" in arr[0]):\n",
    "            base_num=3\n",
    "        else:\n",
    "            return 0,0\n",
    "        # else remains 0, for Relaxation\n",
    "        trial_no=extract_number(arr[-1])\n",
    "        if trial_no==2:\n",
    "            base_num+=3\n",
    "        elif trial_no==3:\n",
    "            base_num+=6\n",
    "        return int(arr[-2]),base_num # returns address of cell in excel file\n",
    "\n",
    "    # mapping the stress ratings to either 0 or 1 - if rating from 0 to 5, marked as low stress, if rating from 6-10, marked as high stress\n",
    "    def diff_label(stress_label):\n",
    "        if stress_label>=1 and stress_label<=5:\n",
    "            stress_label=0\n",
    "        else:\n",
    "            stress_label=1\n",
    "        return stress_label\n",
    "\n",
    "    # reads the data and returns the samples array and its label array\n",
    "    def read_data(file_paths):\n",
    "        # load first element\n",
    "        data = scipy.io.loadmat(file_paths[0])\n",
    "        eeg_data = data[\"Clean_data\"]\n",
    "        all_samples=[eeg_data]\n",
    "        addr1,addr2=extract_label_address(file_paths[0])\n",
    "        all_labels=[diff_label(stress_levels_arr[addr1][addr2])]\n",
    "        for i in range(1,len(file_paths)):\n",
    "            data = scipy.io.loadmat(file_paths[i])# loads EEG coordinates from .mat file\n",
    "            eeg_data = data[\"Clean_data\"] # get EEG data in numpy array, has 32 channels, and 3200 time samples\n",
    "            all_samples=np.append(all_samples,[eeg_data],axis=0)\n",
    "            # extract the stress level for that one file of EEG data\n",
    "            addr1,addr2=extract_label_address(file_paths[i])\n",
    "            all_labels=np.append(all_labels,[diff_label(stress_levels_arr[addr1][addr2])],axis=0)\n",
    "        return all_samples,all_labels\n",
    "\n",
    "The dataset contains some EEG samples called \"Relax\", which represents\n",
    "the non-stress EEG samples. LR2 is only required to differentiate\n",
    "between low stress and high stress EEG samples, therefore the filepaths\n",
    "to the Relax EEG samples are removed.\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    # total 480 files, 3 trials for each of the 40 participants in each of the 4 experiments\n",
    "    all_file_paths=sorted(glob(\"filtered_data/*.mat\"))\n",
    "    # Filter out file names containing the word \"Relax\" - we are not using non-stress EEG samples\n",
    "    filtered_file_paths = [file_path for file_path in all_file_paths if \"Relax\" not in file_path]\n",
    "    all_file_paths=filtered_file_paths\n",
    "\n",
    "    # reading excel file's stress level labels\n",
    "    stress_levels_arr = pd.read_excel('Dataset_Information/scales.xls').to_numpy()\n",
    "\n",
    "    # each sample is of shape (32,3200), total 120 samples per experiment\n",
    "    # we do not use the relaxation EEG samples to train the model, so 360 files will be used, not 480\n",
    "    # load the EEG data and its labels\n",
    "    all_samples,all_labels=read_data(all_file_paths) # should be (360,32,3200) and (360,)\n",
    "\n",
    "    # test if labels are correct\n",
    "    print(all_samples.shape,all_labels.shape) # should be (360,32,3200) and (360,)\n",
    "    print(\"File Name: \",all_file_paths[3])\n",
    "    print(\"Label: \",all_labels[3])\n",
    "\n",
    "    (360, 32, 3200) (360,)\n",
    "    File Name:  filtered_data\\Arithmetic_sub_11_trial1.mat\n",
    "    Label:  1\n",
    "\n",
    "Structure of all_samples array: Its shape is (360,32,3200) signifying\n",
    "360 files, each containing 32 channels worth of 3200 time samples\n",
    "\n",
    "**2. Preprocessing Data for LR Model**\n",
    "\n",
    "-   Although the EEG data samples are already preprocessed in the sense\n",
    "    of artifact and noise removal, there are additional preprocessing\n",
    "    steps which include:\n",
    "    -   oversampling to balance the classes ( There are 233 low-stress\n",
    "        EEG samples and 127 high-stress EEG samples. Therefore random\n",
    "        oversampling had to be done for high-stress EEG samples to\n",
    "        compensate for the class imbalance.)\n",
    "    -   splitting the data into training and testing sets (80% of the\n",
    "        dataset was used for training, 20% was used for testing)\n",
    "    -   flattening the data along the channel axis to prepare it for\n",
    "        input into the LR model.\n",
    "\n",
    "**3. Training the Model**\n",
    "\n",
    "-   LR2 shares the same training methodologies as LR1 - refer to\n",
    "    LR1_Model_Documentation.html\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    # Assuming all_samples shape is (360, 32, 3200)\n",
    "    # Assuming all_labels shape is (360,)\n",
    "\n",
    "    # identify indices of high-stress class\n",
    "    high_stress_indices = np.where(all_labels == 1)[0]\n",
    "\n",
    "    # oversampling factor\n",
    "    oversampling_factor = 3\n",
    "\n",
    "    # replicate high-stress samples\n",
    "    replicated_samples = np.repeat(all_samples[high_stress_indices], oversampling_factor, axis=0)\n",
    "    replicated_labels = np.repeat(all_labels[high_stress_indices], oversampling_factor)\n",
    "\n",
    "    # combine with original data\n",
    "    oversampled_samples = np.vstack([all_samples, replicated_samples])\n",
    "    oversampled_labels = np.concatenate([all_labels, replicated_labels])\n",
    "\n",
    "    # split oversampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(oversampled_samples, oversampled_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # flatten data along the channel axis\n",
    "    X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flatten = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    # create Logistic Regression model\n",
    "    logistic_model = LogisticRegression(random_state=42)\n",
    "\n",
    "    # train model on training set\n",
    "    logistic_model.fit(X_train_flatten, y_train)\n",
    "\n",
    "Out\\[6\\]:\n",
    "\n",
    "    LogisticRegression(random_state=42)\n",
    "\n",
    "**4. Testing the Model - confusion matrix and classification report**\n",
    "\n",
    "The model is tested in the code section below. Report generated in the\n",
    "output provides a summary of various classification metrics such as\n",
    "precision, recall, F1-score, and support for each class. This reports\n",
    "the evaluation of testing using 20% of the data from the dataset.\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    # make predictions on test set\n",
    "    predictions = logistic_model.predict(X_test_flatten)\n",
    "\n",
    "    # evaluate model's performance\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy = accuracy *100\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    classification_rep = classification_report(y_test, predictions)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"Classification Report:\\n\", classification_rep)\n",
    "    print(\"Training Set - Class Distribution:\", np.bincount(y_train))\n",
    "    print(\"Testing Set - Class Distribution:\", np.bincount(y_test))\n",
    "\n",
    "    Accuracy: 93.28859060402685\n",
    "    Confusion Matrix:\n",
    "     [[43 10]\n",
    "     [ 0 96]]\n",
    "    Classification Report:\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "               0       1.00      0.81      0.90        53\n",
    "               1       0.91      1.00      0.95        96\n",
    "\n",
    "        accuracy                           0.93       149\n",
    "       macro avg       0.95      0.91      0.92       149\n",
    "    weighted avg       0.94      0.93      0.93       149\n",
    "\n",
    "    Training Set - Class Distribution: [180 412]\n",
    "    Testing Set - Class Distribution: [53 96]\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    # simple testing of trained model - check if predicts accurately for any random EEG data sample in dataset\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.io import loadmat\n",
    "\n",
    "    # load EEG data from the .mat file - example\n",
    "\n",
    "    test_index=3\n",
    "\n",
    "    new_user_input = loadmat(all_file_paths[test_index])   # loads eeg data at index 359 for preidiction\n",
    "    eeg_data = new_user_input[\"Clean_data\"]\n",
    "\n",
    "    # flatten the input data along the channel axis\n",
    "    selected_data = eeg_data\n",
    "    input_for_model = selected_data.reshape(1, -1)\n",
    "    input_for_model.shape\n",
    "\n",
    "    # make predictions\n",
    "    prediction = logistic_model.predict(input_for_model)\n",
    "\n",
    "    print(\"Actual Label:\", all_labels[test_index])   # checks the actual label at index 359\n",
    "    print(f'Predicted Label: {prediction[0]}')\n",
    "\n",
    "    Actual Label: 1\n",
    "    Predicted Label: 1\n",
    "\n",
    "**5. Saving the Trained Model**\n",
    "\n",
    "This action ensures that the trained model can be easily retrieved and\n",
    "reused for future predictions or analysis without the need to retrain\n",
    "it. The .h5 version will be used when the model is required to classify\n",
    "a user's EEG sample for the website.\n",
    "\n",
    "In \\[39\\]:\n",
    "\n",
    "    import joblib\n",
    "    import h5py\n",
    "\n",
    "    # Save the trained Logistic Regression model using joblib\n",
    "    joblib.dump(logistic_model, 'C:/Users/Manoharan/Desktop/LR2.h5')\n",
    "\n",
    "Out\\[39\\]:\n",
    "\n",
    "    ['C:/Users/Manoharan/Desktop/LR2.h5']"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
